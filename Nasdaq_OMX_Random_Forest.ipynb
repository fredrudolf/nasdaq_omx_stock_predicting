{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Manually collecting the data\n",
    "1. In the jupyter notebook root folder, create a new folder called \"omxs30_data\"\n",
    "2. Go to http://www.nasdaqomxnordic.com/aktier/historiskakurser\n",
    "3. Download data for wanted stocks and place the csv files in \"omxs30_data\"\n",
    "\n",
    "To fully recreate the same experiment, use the same date settings and download the same stocks.\n",
    "The list of stocks and selected dates can be seen in the output of cell 2 in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# technical analysis library\n",
    "import talib\n",
    "\n",
    "# sklearn to develop a model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# easy handling of data\n",
    "import pandas as pd\n",
    "# to make number crunching easy and fast\n",
    "import numpy as np\n",
    "np.random.seed(55)\n",
    "\n",
    "# to allow file handling\n",
    "import os\n",
    "\n",
    "# pretty print!\n",
    "from pprint import pprint\n",
    "\n",
    "# to handle dates\n",
    "import ciso8601\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks (including dates) used:\n",
      "['ABB-1900-01-01-2017-08-03.csv',\n",
      " 'ALFA-1900-01-01-2017-08-03.csv',\n",
      " 'ALIV-SDB-1900-01-01-2017-08-03.csv',\n",
      " 'ASSA-B-1900-01-01-2017-08-03.csv',\n",
      " 'ATCO-B-1900-01-01-2017-08-03.csv',\n",
      " 'AZN-1900-01-01-2017-08-03.csv',\n",
      " 'BOL-1900-01-01-2017-08-03.csv',\n",
      " 'ELUX-B-1900-01-01-2017-08-03.csv',\n",
      " 'ERIC-B-1900-01-01-2017-08-03.csv',\n",
      " 'GETI-B-1900-01-01-2017-08-03.csv',\n",
      " 'HM-B-1900-01-01-2017-08-03.csv',\n",
      " 'INVE-B-1900-01-01-2017-08-03.csv',\n",
      " 'KINV-B-1900-01-01-2017-08-03.csv',\n",
      " 'LUPE-1900-01-01-2017-08-03.csv',\n",
      " 'NDA-SEK-1900-01-01-2017-08-03.csv',\n",
      " 'SAND-1900-01-01-2017-08-03.csv',\n",
      " 'SCA-A-1900-01-01-2017-08-03.csv',\n",
      " 'SEB-A-1900-01-01-2017-08-03.csv',\n",
      " 'SECU-B-1900-01-01-2017-08-03.csv',\n",
      " 'SKF-B-1900-01-01-2017-08-03.csv',\n",
      " 'SSAB-A-1900-01-01-2017-08-03.csv',\n",
      " 'SWED-A-1900-01-01-2017-08-03.csv',\n",
      " 'SWMA-1900-01-01-2017-08-03.csv',\n",
      " 'TEL2-B-1900-01-01-2017-08-03.csv',\n",
      " 'TELIA-1900-01-01-2017-08-03.csv',\n",
      " 'VOLV-B-1900-01-01-2017-08-03.csv']\n"
     ]
    }
   ],
   "source": [
    "## Automatically select all csv files in a folder\n",
    "\n",
    "# folder name\n",
    "directory_name = \"omxs30_data\"\n",
    "file_list = os.listdir(directory_name)\n",
    "# Remove all files that doesn't end with .csv\n",
    "csv_file_list = list(filter(lambda file_name: file_name.split(\".\")[-1] == \"csv\", file_list))\n",
    "# To reproduce the very same experiment, use the same stocks, as well as the same dates.\n",
    "print('Stocks (including dates) used:')\n",
    "pprint(csv_file_list)\n",
    "# Add directory prefix\n",
    "stock_file_list = [directory_name+\"/\"+file_name for file_name in csv_file_list]\n",
    "\n",
    "\n",
    "# Uncomment row below to test with only one stock\n",
    "#stock_file_list = [directory_name+\"/SWMA-1900-01-01-2017-08-03.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FEATURE ENGINEERING\n",
    "# CBBL1 = Close/BBL(20,2)\n",
    "# CBBL2 = Close/BBL(5,2)\n",
    "# BBW1 = BBW(20,2)\n",
    "# BBW2 = BBW(5,2)\n",
    "# CSMA1 = Close/SMA(200)\n",
    "# CSMA2 = Close/SMA(3)\n",
    "# TSMA1 = Turnover/SMA(200)\n",
    "# TSMA2 = Turnover/SMA(3)\n",
    "# HL = High/low\n",
    "# CL = Close/low\n",
    "# HC = High/Close\n",
    "# CC-1 = Close(t)/Close(t-1)\n",
    "\n",
    "# FEATURE IDEAS: Day of the week, Day of the month, Month\n",
    "def featureEngineer(stock_file_list):\n",
    "    df_dict = {}\n",
    "    for stock_file_name in stock_file_list:\n",
    "        # Get the unique stock ticker\n",
    "        ticker = stock_file_name.split(\"-\")[0].split(\"/\")[-1]\n",
    "\n",
    "        # Read the csv file into a pandas dataframe\n",
    "        df = pd.read_csv(stock_file_name, header=1, delimiter=\";\", decimal=\",\")\n",
    "        # The \"Unnamed: 11\" column looks like an all through NaN field. Let's remove it\n",
    "        df1 = df.dropna(axis=1, how=\"all\", inplace=False)\n",
    "\n",
    "        # Some other values are also set to NaN. \"Bid\", \"Ask\", \"Opening price\" and \"Average price\".\n",
    "        # Are those fields needed? For now, no.\n",
    "\n",
    "        # Option 1: Remove all the columns (\"Bid\", \"Ask\" etc)\n",
    "        df2 = df1.drop([\"Bid\", \"Ask\", \"Opening price\", \"Average price\"], axis=1, inplace=False)\n",
    "\n",
    "        # Option 2: Remove all the days with any NaN value.\n",
    "        # The NaN days are assumed to be connected (otherwise the integrity of the data is at risk)\n",
    "        #df2 = df1.dropna(axis=0, how=\"any\", inplace=False)\n",
    "        #Remove rows where a value is <=0. All values should under normal conditions be >0.\n",
    "        #df = df.loc[df['Opening price'] > 0]\n",
    "\n",
    "        # talib needs the days in ascending order\n",
    "        close = df2['Closing price']\n",
    "        close = np.flipud(close)\n",
    "\n",
    "\n",
    "        # Moving average (unweighted rolling mean) on close price with a time period of 200 days \n",
    "        smaperiod1=200\n",
    "        sma1 = talib.SMA(close, timeperiod=smaperiod1)\n",
    "        csma1 = close/sma1\n",
    "        csma1 = np.flipud(csma1)\n",
    "\n",
    "        # Moving average (unweighted rolling mean) on close price with a time period of 3 days \n",
    "        smaperiod2=3\n",
    "        sma2 = talib.SMA(close, timeperiod=smaperiod2)\n",
    "        csma2 = close/sma2\n",
    "        csma2 = np.flipud(csma2)\n",
    "\n",
    "\n",
    "        # Bollinger band upper, middle, lower and width for BollingerBand(20,2)\n",
    "        bbu1, bbm1, bbl1 = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)\n",
    "        cbbl1 = close/bbl1\n",
    "        cbbl1 = np.flipud(cbbl1)\n",
    "\n",
    "        bbw1 = bbu1/bbl1\n",
    "        bbw1 = np.flipud(bbw1)\n",
    "\n",
    "\n",
    "        # Bollinger band upper, middle, lower and width for BollingerBand(5,2)\n",
    "        bbu2, bbm2, bbl2 = talib.BBANDS(close, timeperiod=5, nbdevup=2, nbdevdn=2)\n",
    "\n",
    "        cbbl2 = close/bbl2\n",
    "        cbbl2 = np.flipud(cbbl2)\n",
    "\n",
    "        bbw2 = bbu2/bbl2\n",
    "        bbw2 = np.flipud(bbw2)\n",
    "\n",
    "\n",
    "        # Moving average (unweighted rolling mean) on turnover with a time period of 200 days\n",
    "        # Replace missing values with mean\n",
    "        df2['Turnover'].fillna(df2['Turnover'].mean())\n",
    "        turnover = df2['Turnover']\n",
    "        turnover = np.flipud(turnover)\n",
    "\n",
    "        turnoverperiod1=200\n",
    "        turnover1 = talib.SMA(turnover, timeperiod=turnoverperiod1)\n",
    "        turnover1 = np.array(turnover1)\n",
    "        tsma1 = turnover/turnover1\n",
    "        tsma1 = np.flipud(tsma1)\n",
    "\n",
    "        # Moving average (unweighted rolling mean) on turnover with a time period of 3 days\n",
    "        turnoverperiod2=3\n",
    "        turnover2 = talib.SMA(turnover, timeperiod=turnoverperiod2)\n",
    "        turnover2 = np.array(turnover2)\n",
    "        tsma2 = turnover/turnover2\n",
    "        tsma2 = np.flipud(tsma2)\n",
    "\n",
    "        # What's the closing price 1 day earlier?\n",
    "        closePrev = close/np.concatenate(([np.nan],close[:-1]))\n",
    "        closePrev = np.flipud(closePrev)\n",
    "\n",
    "        ## LABEL\n",
    "        # CC1 = Close(t+1)/Close(t) <---- outcome\n",
    "        # What's the closing price 1 day later?\n",
    "        close1 = np.concatenate((close[1:],[np.nan]))/close\n",
    "        close1 = np.flipud(close1)\n",
    "\n",
    "\n",
    "        # create a dataframe using the features created above\n",
    "        features_df = pd.DataFrame({\"CSMA1\":csma1, \"CSMA2\":csma2,\n",
    "                                    #\"TSMA1\":tsma1, \"TSMA2\":tsma2, # <-- caused a bug, not all dates were present\n",
    "                                    \"CBBL1\":cbbl1, \"BBW1\":bbw1,\n",
    "                                    \"CBBL2\":cbbl2, \"BBW2\":bbw2,\n",
    "                                    \"CC-1\":closePrev,\n",
    "                                    \"CC1\":close1,\n",
    "                                   })\n",
    "\n",
    "        features_df['HL'] = df2['High price'].as_matrix()/df2['Low price'].as_matrix()\n",
    "        features_df['CL'] = df2['Closing price'].as_matrix()/df2['Low price'].as_matrix()\n",
    "        features_df['HC'] = df2['High price'].as_matrix()/df2['Closing price'].as_matrix()\n",
    "\n",
    "        # add the feature dataframe to the existing dataframe\n",
    "        df3 = pd.concat([df2, features_df], axis=1)\n",
    "\n",
    "        # Remove days with NaN fields\n",
    "        # WARNING: This removes any field NaN is present. E.g., CC1 and CC-1 contains some NaN padding\n",
    "        df4 = df3.dropna(axis=0, how=\"any\", inplace=False)\n",
    "\n",
    "\n",
    "        df_dict[ticker] = df4    \n",
    "    return df_dict\n",
    "\n",
    "df_dict = featureEngineer(stock_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Put all the data in one single dataframe (instead of separating them per stock)\n",
    "all_stocks_df = pd.DataFrame()\n",
    "for stockName, df_entry in df_dict.items():\n",
    "    all_stocks_df = pd.concat([all_stocks_df, df_entry], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set 2 is 4.44% of the whole dataset\n"
     ]
    }
   ],
   "source": [
    "## create test set 2\n",
    "# get the date which is testset2_nr_of_days before last date\n",
    "testset2_nr_of_days = 365\n",
    "last_date = all_stocks_df['Date'].max()\n",
    "testset2_beginning_datetime = ciso8601.parse_datetime(last_date)-datetime.timedelta(days=testset2_nr_of_days)\n",
    "testset2_beginning = str(testset2_beginning_datetime)[:-9]\n",
    "\n",
    "# separate the test set 2 data from the rest of the dataset\n",
    "testset2_df = all_stocks_df[all_stocks_df['Date'] > testset2_beginning]\n",
    "rest_df = all_stocks_df[all_stocks_df['Date'] <= testset2_beginning]\n",
    "print(\"Test set 2 is %.2f\" % (len(testset2_df)/len(all_stocks_df)*100) + \"% of the whole dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove columns that shouldn't be used in this model\n",
    "testset2_cc1 = testset2_df.pop(\"CC1\")\n",
    "testset2_date = testset2_df.pop(\"Date\")\n",
    "testset2_df2 = testset2_df.drop([\"Closing price\", \"High price\", \"Low price\", \"Total volume\", \"Turnover\", \"Trades\"], inplace=False, axis=1)\n",
    "\n",
    "rest_df2 = rest_df.drop([\"Closing price\", \"High price\", \"Low price\", \"Total volume\", \"Turnover\", \"Trades\"], inplace=False, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "separateByDate = True # to avoid problems caused by correlation of stocks during the same date\n",
    "\n",
    "# split into a training set and a test set\n",
    "if (separateByDate):\n",
    "    train_df_dates, testset1_df_dates = train_test_split(list(rest_df['Date'].unique()), test_size=0.2, random_state=42)\n",
    "    train_df =  rest_df2[rest_df['Date'].isin(train_df_dates)]\n",
    "    testset1_df =  rest_df2[rest_df['Date'].isin(testset1_df_dates)]\n",
    "else:\n",
    "    train_df, testset1_df = train_test_split(rest_df2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Close to close threshold. 1.005 means >0.5% returns True, otherwise False\n",
    "ccThres = 1.005\n",
    "\n",
    "# Remove the label (outcome)\n",
    "y_cc1 = train_df.pop(\"CC1\")\n",
    "# Date might be needed later\n",
    "train_date = train_df.pop(\"Date\")\n",
    "# Convert outcome to binary outcome: More than 0.5% close-to-close return or not\n",
    "y = y_cc1>ccThres\n",
    "# The rest of train_df should be features. Set this as training set X\n",
    "X = train_df\n",
    "\n",
    "# For later testing\n",
    "y_testset1_cc1 = testset1_df.pop(\"CC1\")\n",
    "testset1_date = testset1_df.pop(\"Date\")\n",
    "y_testset1 = y_testset1_cc1>ccThres\n",
    "X_testset1 = testset1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mbernico/CS570/blob/master/.ipynb_checkpoints/Random_Forests_%3D%3D_Awesome-checkpoint.ipynb\n",
    "#### Thanks Mike Bernico for an excellent random forest tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = True # fit model or load it from file?\n",
    "if (fit_model):\n",
    "    model = RandomForestRegressor(n_estimators=2000, \n",
    "                                  oob_score=True, \n",
    "                                  n_jobs=-1, \n",
    "                                  random_state=42, \n",
    "                                  max_features=\"auto\", \n",
    "                                  min_samples_leaf=10)\n",
    "    model.fit(X, y)\n",
    "else:\n",
    "    model = joblib.load('rf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Average Return (AR): 0.14%\n",
      "Benchmark Standard Deviation (SD): 2.23%\n",
      "Benchmark Sharpe Ratio (SR): 0.06\n",
      "\n",
      "Average Return (AR): 0.47%\n",
      "Standard Deviation (SD): 3.75%\n",
      "Sharpe Ratio (SR): 0.13\n",
      "\n",
      "Nr of buy signals 1349\n",
      "Nr of days with one or more buy signals 659\n",
      "Total nr of days 28238\n"
     ]
    }
   ],
   "source": [
    "# random forest model outputs a value between 0 and 1.\n",
    "# 1 means that all the trees agree that it should be more than a 0.5% gain\n",
    "# 0 means that all the trees agree that it should be equal to or less than 0.5% gain\n",
    "thres = 0.5\n",
    "\n",
    "## TEST SET 1\n",
    "\n",
    "y_test_prediction = list(model.predict(X_testset1))\n",
    "y_test_actual = list(y_testset1)\n",
    "y_test_cc1_actual = list(y_testset1_cc1)\n",
    "\n",
    "test_set_prediction_df = pd.DataFrame({\"Output\":y_test_prediction,\n",
    "                                        \"Date\":testset1_date,\n",
    "                                        \"Outcome\":y_test_cc1_actual})\n",
    "\n",
    "\n",
    "bar = (test_set_prediction_df[\"Outcome\"].mean()*100-100)\n",
    "bsd = (test_set_prediction_df[\"Outcome\"].std()*100)\n",
    "bsr = bar/bsd\n",
    "print('Benchmark Average Return (AR): %.2f' % bar + \"%\")\n",
    "print('Benchmark Standard Deviation (SD): %.2f' % bsd + \"%\")\n",
    "print('Benchmark Sharpe Ratio (SR): %.2f' % bsr)\n",
    "print()\n",
    "# m\n",
    "test_set_buy_signals = test_set_prediction_df[test_set_prediction_df[\"Output\"] > thres]\n",
    "ar = (test_set_buy_signals[\"Outcome\"].mean()*100-100)\n",
    "sd = (test_set_buy_signals[\"Outcome\"].std()*100)\n",
    "sr = ar/sd\n",
    "print('Average Return (AR): %.2f' % ar + \"%\")\n",
    "print('Standard Deviation (SD): %.2f' % sd + \"%\")\n",
    "print('Sharpe Ratio (SR): %.2f' % sr)\n",
    "print()\n",
    "print('Nr of buy signals %i' % len(test_set_buy_signals))\n",
    "print('Nr of days with one or more buy signals %i' % len(test_set_buy_signals[\"Date\"].unique()))\n",
    "print('Total nr of days %i' % len(test_set_prediction_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Average Return (AR): 0.06%\n",
      "Benchmark Standard Deviation (SD): 1.66%\n",
      "Benchmark Sharpe Ratio (SR): 0.04\n",
      "\n",
      "Average Return (AR): 0.34%\n",
      "Standard Deviation (SD): 1.74%\n",
      "Sharpe Ratio (SR): 0.19\n",
      "\n",
      "Nr of buy signals 63\n",
      "Nr of days with one or more buy signals 53\n",
      "Total nr of days 6578\n"
     ]
    }
   ],
   "source": [
    "#TEST SET 2 - the real deal\n",
    "\n",
    "test_set_output = model.predict(testset2_df2)\n",
    "test_set_prediction_df = pd.DataFrame({\"Output\":test_set_output,\n",
    "                                        \"Date\":testset2_date,\n",
    "                                        \"Outcome\":testset2_cc1})\n",
    "\n",
    "bar = (test_set_prediction_df[\"Outcome\"].mean()*100-100)\n",
    "bsd = (test_set_prediction_df[\"Outcome\"].std()*100)\n",
    "bsr = bar/bsd\n",
    "print('Benchmark Average Return (AR): %.2f' % bar + \"%\")\n",
    "print('Benchmark Standard Deviation (SD): %.2f' % bsd + \"%\")\n",
    "print('Benchmark Sharpe Ratio (SR): %.2f' % bsr)\n",
    "print()\n",
    "# m\n",
    "test_set_buy_signals = test_set_prediction_df[test_set_prediction_df[\"Output\"] > thres]\n",
    "ar = (test_set_buy_signals[\"Outcome\"].mean()*100-100)\n",
    "sd = (test_set_buy_signals[\"Outcome\"].std()*100)\n",
    "sr = ar/sd\n",
    "print('Average Return (AR): %.2f' % ar + \"%\")\n",
    "print('Standard Deviation (SD): %.2f' % sd + \"%\")\n",
    "print('Sharpe Ratio (SR): %.2f' % sr)\n",
    "print()\n",
    "print('Nr of buy signals %i' % len(test_set_buy_signals))\n",
    "print('Nr of days with one or more buy signals %i' % len(test_set_buy_signals[\"Date\"].unique()))\n",
    "print('Total nr of days %i' % len(test_set_prediction_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered mean 1.53%\n",
      "Non clustered mean -0.18%\n"
     ]
    }
   ],
   "source": [
    "# testing the compounded interest for the signals produced by test set 2\n",
    "# some of the signals are on the same date, which is taken care of by\n",
    "# allocating an equal portion of the fictive portfolio\n",
    "\n",
    "date_sorted_buy_signals = test_set_buy_signals.sort_values(by=\"Date\", ascending=False)\n",
    "\n",
    "trade_results = []\n",
    "non_clustered_signal_results = []\n",
    "clustered_signal_results = []\n",
    "for date in date_sorted_buy_signals[\"Date\"].unique():\n",
    "    trades_on_date = test_set_buy_signals[test_set_buy_signals[\"Date\"] == date]\n",
    "    allocation = len(trades_on_date)\n",
    "    individual_trade_results = []\n",
    "    for index, trade in trades_on_date.iterrows():\n",
    "        individual_trade_results.append(trade[\"Outcome\"]/allocation)\n",
    "        if (len(trades_on_date) > 1):\n",
    "            clustered_signal_results.append(trade[\"Outcome\"])\n",
    "        else:\n",
    "            non_clustered_signal_results.append(trade[\"Outcome\"])\n",
    "    trade_results.append(np.sum(individual_trade_results))\n",
    "trade_results\n",
    "compounded = 1.0\n",
    "for result in trade_results:\n",
    "    compounded *= result\n",
    "compounded\n",
    "\n",
    "clustered_outcome_mean = (np.mean(clustered_signal_results)*100-100)\n",
    "non_clustered_outcome_mean = (np.mean(non_clustered_signal_results)*100-100)\n",
    "\n",
    "print(\"Clustered mean %.2f\" % clustered_outcome_mean + \"%\")\n",
    "print(\"Non clustered mean %.2f\" % non_clustered_outcome_mean + \"%\")\n",
    "\n",
    "#print(np.max(clustered_signal_results)*100-100)\n",
    "#print(np.min(clustered_signal_results)*100-100)\n",
    "#print(np.max(non_clustered_signal_results)*100-100)\n",
    "#print(np.min(non_clustered_signal_results)*100-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "histnorm": "probability",
         "name": "Clustered signals",
         "opacity": 0.75,
         "type": "histogram",
         "x": [
          -0.10378827192526785,
          1.8567639257294246,
          1.2830188679245254,
          0.6118881118881063,
          0.3041362530413494,
          -0.14272121788772552,
          -0.28598665395615797,
          -0.6399069226294358,
          2.797202797202786,
          1.2465373961218802,
          -0.7198560287942257,
          4.869913275517007,
          2.9914529914529737,
          2.2277227722772324,
          0.9003215434083529,
          1.9966015293118318,
          0.7733007733007753,
          3.0024019215372277,
          6.04956268221575
         ],
         "xbins": {
          "end": 10,
          "size": 1,
          "start": -10
         }
        },
        {
         "histnorm": "probability",
         "name": "Non clustered signals",
         "opacity": 0.75,
         "type": "histogram",
         "x": [
          1.7747858017135911,
          0.3586800573888098,
          0.9781357882623638,
          0.4859086491739504,
          -2.508250825082513,
          0.4836759371221149,
          -0.41580041580041893,
          0,
          0.7042253521126725,
          -0.17817371937638882,
          0.07209805335257613,
          -2.202339986235387,
          -1.4293567894447534,
          -0.7365439093484554,
          -0.04184100418409287,
          2.102607232968893,
          -2.5409836065573614,
          -2.6336791699920212,
          -0.6752954417557646,
          -0.09119927040583775,
          -0.9118541033434724,
          -0.8282716731087874,
          0.10380622837371334,
          2.1923076923076934,
          0.5981511691136348,
          -1.1675264729839796,
          -1.5126050420168013,
          -2.3255813953488484,
          -1.0168084664868218,
          1.2226512226512227,
          1.214574898785429,
          -0.8317929759704299,
          -1.095617529880471,
          0.6465517241379217,
          2.588235294117652,
          0.8367303153829653,
          -0.23386342376052482,
          1.9113149847094917,
          -0.6293266205160535,
          -0.5169561621174523,
          1.5204054414510466,
          -3.1653477717617733,
          -1.6646366220056876,
          1.7449664429530287
         ],
         "xbins": {
          "end": 10,
          "size": 1,
          "start": -10
         }
        }
       ],
       "layout": {
        "barmode": "overlay",
        "title": "Distribution of buy signal outcomes",
        "xaxis": {
         "title": "Outcome [%]"
        },
        "yaxis": {
         "title": "Probability"
        }
       }
      },
      "text/html": [
       "<div id=\"5c56d31d-5288-4e94-8e5e-b9673b884cc7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5c56d31d-5288-4e94-8e5e-b9673b884cc7\", [{\"name\": \"Clustered signals\", \"xbins\": {\"end\": 10.0, \"start\": -10.0, \"size\": 1}, \"histnorm\": \"probability\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [-0.10378827192526785, 1.8567639257294246, 1.2830188679245254, 0.6118881118881063, 0.3041362530413494, -0.14272121788772552, -0.28598665395615797, -0.6399069226294358, 2.797202797202786, 1.2465373961218802, -0.7198560287942257, 4.869913275517007, 2.9914529914529737, 2.2277227722772324, 0.9003215434083529, 1.9966015293118318, 0.7733007733007753, 3.0024019215372277, 6.04956268221575]}, {\"name\": \"Non clustered signals\", \"xbins\": {\"end\": 10.0, \"start\": -10.0, \"size\": 1}, \"histnorm\": \"probability\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [1.7747858017135911, 0.3586800573888098, 0.9781357882623638, 0.4859086491739504, -2.508250825082513, 0.4836759371221149, -0.41580041580041893, 0.0, 0.7042253521126725, -0.17817371937638882, 0.07209805335257613, -2.202339986235387, -1.4293567894447534, -0.7365439093484554, -0.04184100418409287, 2.102607232968893, -2.5409836065573614, -2.6336791699920212, -0.6752954417557646, -0.09119927040583775, -0.9118541033434724, -0.8282716731087874, 0.10380622837371334, 2.1923076923076934, 0.5981511691136348, -1.1675264729839796, -1.5126050420168013, -2.3255813953488484, -1.0168084664868218, 1.2226512226512227, 1.214574898785429, -0.8317929759704299, -1.095617529880471, 0.6465517241379217, 2.588235294117652, 0.8367303153829653, -0.23386342376052482, 1.9113149847094917, -0.6293266205160535, -0.5169561621174523, 1.5204054414510466, -3.1653477717617733, -1.6646366220056876, 1.7449664429530287]}], {\"title\": \"Distribution of buy signal outcomes\", \"yaxis\": {\"title\": \"Probability\"}, \"xaxis\": {\"title\": \"Outcome [%]\"}, \"barmode\": \"overlay\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5c56d31d-5288-4e94-8e5e-b9673b884cc7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5c56d31d-5288-4e94-8e5e-b9673b884cc7\", [{\"name\": \"Clustered signals\", \"xbins\": {\"end\": 10.0, \"start\": -10.0, \"size\": 1}, \"histnorm\": \"probability\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [-0.10378827192526785, 1.8567639257294246, 1.2830188679245254, 0.6118881118881063, 0.3041362530413494, -0.14272121788772552, -0.28598665395615797, -0.6399069226294358, 2.797202797202786, 1.2465373961218802, -0.7198560287942257, 4.869913275517007, 2.9914529914529737, 2.2277227722772324, 0.9003215434083529, 1.9966015293118318, 0.7733007733007753, 3.0024019215372277, 6.04956268221575]}, {\"name\": \"Non clustered signals\", \"xbins\": {\"end\": 10.0, \"start\": -10.0, \"size\": 1}, \"histnorm\": \"probability\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [1.7747858017135911, 0.3586800573888098, 0.9781357882623638, 0.4859086491739504, -2.508250825082513, 0.4836759371221149, -0.41580041580041893, 0.0, 0.7042253521126725, -0.17817371937638882, 0.07209805335257613, -2.202339986235387, -1.4293567894447534, -0.7365439093484554, -0.04184100418409287, 2.102607232968893, -2.5409836065573614, -2.6336791699920212, -0.6752954417557646, -0.09119927040583775, -0.9118541033434724, -0.8282716731087874, 0.10380622837371334, 2.1923076923076934, 0.5981511691136348, -1.1675264729839796, -1.5126050420168013, -2.3255813953488484, -1.0168084664868218, 1.2226512226512227, 1.214574898785429, -0.8317929759704299, -1.095617529880471, 0.6465517241379217, 2.588235294117652, 0.8367303153829653, -0.23386342376052482, 1.9113149847094917, -0.6293266205160535, -0.5169561621174523, 1.5204054414510466, -3.1653477717617733, -1.6646366220056876, 1.7449664429530287]}], {\"title\": \"Distribution of buy signal outcomes\", \"yaxis\": {\"title\": \"Probability\"}, \"xaxis\": {\"title\": \"Outcome [%]\"}, \"barmode\": \"overlay\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this to see the difference between\n",
    "# non clustered and clustered buy signals\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "trace1 = go.Histogram(\n",
    "    x=np.array(clustered_signal_results)*100-100,\n",
    "    opacity=0.75,\n",
    "    histnorm='probability',\n",
    "    name='Clustered signals',\n",
    "    xbins=dict(\n",
    "        start=-10.0,\n",
    "        end=10.0,\n",
    "        size=1\n",
    "    )\n",
    ")\n",
    "trace2 = go.Histogram(\n",
    "    x=np.array(non_clustered_signal_results)*100-100,\n",
    "    opacity=0.75,\n",
    "    histnorm='probability',\n",
    "    name='Non clustered signals',\n",
    "    xbins=dict(\n",
    "        start=-10.0,\n",
    "        end=10.0,\n",
    "        size=1\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(barmode='overlay', title='Distribution of buy signal outcomes', \n",
    "                  xaxis=dict(\n",
    "        title='Outcome [%]',\n",
    "        ),\n",
    "                  yaxis=dict(\n",
    "        title='Probability',\n",
    "       ))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='overlaid histogram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
